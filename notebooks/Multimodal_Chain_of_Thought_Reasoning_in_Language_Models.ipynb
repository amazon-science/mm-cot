{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h3ppmVJC84c"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/gianfrancodemarco/mm-cot\n",
        "!pip install -r mm-cot/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "\n",
        "def download_file(url, local_filename=\"file.foo\"):\n",
        "  # Streaming, so we can iterate over the response.\n",
        "  response = requests.get(url, stream=True)\n",
        "  total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
        "  block_size = 1024 #1 Kibibyte\n",
        "  progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
        "  with open(local_filename, 'wb') as file:\n",
        "      for data in response.iter_content(block_size):\n",
        "          progress_bar.update(len(data))\n",
        "          file.write(data)\n",
        "  progress_bar.close()\n",
        "  if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
        "      print(\"ERROR, something went wrong\")\n",
        "\n",
        "def unzip(file, dest): \n",
        "  with zipfile.ZipFile(file, 'r') as zip_ref:\n",
        "      zip_ref.extractall(dest)\n",
        "\n",
        "# Download data\n",
        "!git clone https://github.com/lupantech/ScienceQA\n",
        "!mv ScienceQA/data data\n",
        "!rm -r ScienceQA\n",
        "\n",
        "# Download vision features\n",
        "download_file(\n",
        "    'https://drive.google.com/u/0/uc?id=13B0hc_F_45-UlqPLKSgRz-ALtFQ8kIJr&export=download&confirm=t&uuid=a234e603-9c79-4123-9d35-09a326c923d9&at=ALgDtsyNOCvILf1Ri9_B4KcZzjez:1676315895503',\n",
        "    'vision_features.zip'\n",
        ")\n",
        "\n",
        "os.mkdir('vision_features')\n",
        "unzip('vision_features.zip', '.')\n",
        "\n",
        "# Download models\n",
        "download_file(\n",
        "    'https://drive.google.com/u/0/uc?id=1FtTYOJPHnWnFfCxNC6M3gar4RAX5E21b&export=download&confirm=t&uuid=a5d17c71-252a-421b-9fd0-6ce609da7947&at=ALgDtsxrRDAAnvFetgzK3lXuZ9I_:1676314225641',\n",
        "    'models.zip'\n",
        ")\n",
        "\n",
        "os.mkdir('models')\n",
        "unzip('models.zip', '.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gECC4wCwEGL0",
        "outputId": "b43c4ad9-47d4-4161-d459-38ebc76c07bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ScienceQA'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 40 (delta 4), reused 35 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (40/40), 10.75 MiB | 8.14 MiB/s, done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.32G/1.32G [00:26<00:00, 50.6MiB/s]\n",
            "100%|██████████| 1.66G/1.66G [00:30<00:00, 54.1MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__s8LrEUgUkx",
        "outputId": "f901b3e7-2818-435a-f370-b76b67684bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rationale generation\n",
        "!CUDA_VISIBLE_DEVICES=0,1 python mm-cot/main.py \\\n",
        "    --model allenai/unifiedqa-t5-base \\\n",
        "    --user_msg rationale --img_type detr \\\n",
        "    --bs 8 --eval_bs 2 --eval_acc 10 --output_len 512 \\\n",
        "    --final_eval --prompt_format QCM-LE \\\n",
        "    --evaluate_dir models/MM-CoT-UnifiedQA-base-Rationale"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNTu6CNCIVB6",
        "outputId": "342f9ae7-ef0f-41e7-becd-213da95209ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-20 15:54:34.216895: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-20 15:54:35.072703: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 15:54:35.072821: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 15:54:35.072852: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "args Namespace(bs=8, caption_file='data/captions.json', data_root='data', epoch=20, eval_acc=10, eval_bs=2, eval_le=None, evaluate_dir='models/MM-CoT-UnifiedQA-base-Rationale', final_eval=True, img_type='detr', input_len=512, lr=5e-05, model='allenai/unifiedqa-t5-base', options=['A', 'B', 'C', 'D', 'E'], output_dir='experiments', output_len=512, prompt_format='QCM-LE', seed=42, test_le=None, test_split='test', train_split='train', use_caption=False, use_generate=False, user_msg='rationale', val_split='val')\n",
            "====Input Arguments====\n",
            "{\n",
            "  \"data_root\": \"data\",\n",
            "  \"output_dir\": \"experiments\",\n",
            "  \"model\": \"allenai/unifiedqa-t5-base\",\n",
            "  \"options\": [\n",
            "    \"A\",\n",
            "    \"B\",\n",
            "    \"C\",\n",
            "    \"D\",\n",
            "    \"E\"\n",
            "  ],\n",
            "  \"epoch\": 20,\n",
            "  \"lr\": 5e-05,\n",
            "  \"bs\": 8,\n",
            "  \"input_len\": 512,\n",
            "  \"output_len\": 512,\n",
            "  \"eval_bs\": 2,\n",
            "  \"eval_acc\": 10,\n",
            "  \"train_split\": \"train\",\n",
            "  \"val_split\": \"val\",\n",
            "  \"test_split\": \"test\",\n",
            "  \"use_generate\": false,\n",
            "  \"final_eval\": true,\n",
            "  \"user_msg\": \"rationale\",\n",
            "  \"img_type\": \"detr\",\n",
            "  \"eval_le\": null,\n",
            "  \"test_le\": null,\n",
            "  \"evaluate_dir\": \"models/MM-CoT-UnifiedQA-base-Rationale\",\n",
            "  \"caption_file\": \"data/captions.json\",\n",
            "  \"use_caption\": false,\n",
            "  \"prompt_format\": \"QCM-LE\",\n",
            "  \"seed\": 42\n",
            "}\n",
            "Traceback (most recent call last):\n",
            "  File \"mm-cot/main.py\", line 373, in <module>\n",
            "    problems, qids, name_maps, image_features = load_data_img(args)  # probelms, test question ids, shot example ids\n",
            "  File \"/content/mm-cot/utils_data.py\", line 34, in load_data_img\n",
            "    problems = json.load(open(os.path.join(args.data_root, 'scienceqa/problems.json')))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'data/scienceqa/problems.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace console.log in main.py with print\n",
        "\n",
        "# answer inference\n",
        "!CUDA_VISIBLE_DEVICES=0,1 python mm-cot/main.py \\\n",
        "    --model allenai/unifiedqa-t5-base \\\n",
        "    --user_msg answer --img_type detr \\\n",
        "    --bs 8 --eval_bs 4 --eval_acc 10 --output_len 64 \\\n",
        "    --final_eval --prompt_format QCMG-A \\\n",
        "    --eval_le models/MM-CoT-UnifiedQA-base-Rationale/predictions_ans_eval.json \\\n",
        "    --test_le models/MM-CoT-UnifiedQA-base-Rationale/predictions_ans_test.json \\\n",
        "    --evaluate_dir models/MM-CoT-UnifiedQA-base-Answer"
      ],
      "metadata": {
        "id": "_71m8s9qShLL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}